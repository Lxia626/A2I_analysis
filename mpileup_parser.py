#! /usr/bin/env python3

## the script is used for analyzing A2I editing
import sys, os, re, numpy as np
sys.path.append('/public/home/Songlab/xial/scripts/analysis/')
from collections import Counter
from time import time
from itertools import starmap, filterfalse
from pysam import FastaFile, VariantFile
# from A2I_analysis import main as A2I_downstream
from scipy.stats import binom

def arg(argv):

    import argparse
    parser = argparse.ArgumentParser(description="Candidate editing sites calling", formatter_class=argparse.RawTextHelpFormatter)
    parser.add_argument('-i', '--input', help="mpileup file generated by samtools mpileup", type=str, required=True)
    parser.add_argument('-o', '--out', help="Output of candidate sites", type=str, required=True)
    parser.add_argument('-d', '--depth', help="Minimum depth for each candidate [2]", type=int, default=2)
    parser.add_argument('-m', '--mismatch', help="Threshold for minimum mismatch number [2]", type=int, default=2)
    # parser.add_argument('--snpdb', help="SNP to filter out (chrom+'\t'+pos). If multiple files are supplied, seperate them with ',' ['']", type=str, default='/public/home/Songlab/xial/Work/database/hg19/SNP_all_dbSNP_1000Genomes_UWash.txt')
    parser.add_argument('--puzzle', help="Maximum ratio of puzzling mpileup information ('*', '+', '-', '^', '$', 'D') at the site [0.2]", type=float, default=0.2)
    parser.add_argument('--ahead', help="How many bases ahead from the target site will be used to check whether there is some deletions concerned with the target site [5]", type=int, default=5)
    parser.add_argument('--DR', help="Maximum ratio of deletions at the target site [0.1] ATTENTION: not useful when 'INDEL' is set", type=float, default=0.1)
    parser.add_argument('--INDEL', help="mpileup results generated by bcftools mpileup for INDEL filtration [None]. Once there is an INDEL in the forward [ahead] region, then ignore the site", type=str, default=None)
    parser.add_argument('--skip', help="Maximum ratio of skipped reads [0.4, if level is specified, then it should be set much higher, such as 0.7]", type=float, default=0.4)
    parser.add_argument('-Q', '--quality', help="Minimum quality threshold [7]", type=int, default=7)
    parser.add_argument('-R', '--ref', help="Reference genome ['/public/home/Songlab/xial/Work/database/hg19/hg19_softmasked.fa']", type=str, default='/public/home/Songlab/xial/Work/database/hg19/hg19_softmasked.fa')
    parser.add_argument('-gtf', help="GTF table file ['/public/home/Songlab/songyl/database/genome/human/GRCh37_release75/hg19_ENSEMBL_all_RNAs.table']", type=str, default='/public/home/Songlab/songyl/database/genome/human/GRCh37_release75/hg19_ENSEMBL_all_RNAs.table')
    parser.add_argument('-sr', '--simple_rep', help="Collection of simple repeats [not need if region=='ALU']", type=str, default=None)
    parser.add_argument('--splice_nt', help="Distance from intron/exon boundary [not need if region=='ALU'] [4]", type=int, default=4)
    parser.add_argument('--BP', help="The minimum distance from the head of the reads towards the target sites. [50, if not filter according to base position, then set as 'NA']", type=str, default=50)
    parser.add_argument('--BP_ratio', help="How many of the reads covers the target site within [BP] nt from the head. [0.05]", type=float, default=0.05)
    parser.add_argument('--buffer', help="Length for flanking sequence of editing candidate site for removing homopolymers [not need if region=='ALU'] [4]", type=int, default=4)
    parser.add_argument('-r', '--region', help="Region to analyse {'ALU', 'NONALU'}", default='ALU', type=str)
    parser.add_argument('-p', help="Multiple processes number [1]", type=int, default=1)
    parser.add_argument('--level', help="Only call editing level. Supply target sites in TXT file, in the format as 'chrom\tpos\tstrand'", type=str, default=None)

    args = parser.parse_args() 
    sys.stderr.write('args: '+str(args)+'\n')
    sys.stderr.write('Sequencing error: '+str(10**((-1)*(round(args.quality/10, 1))))+'\n')
    gtf_info = gtf_table_trans(args.gtf)
    if args.BP != 'NA': BP = int(args.BP)
    else: BP = 'NA'

    sys.stderr.write('\n################### Step 1: loading information #################\n')
    START = time()
    # if args.region != 'ALU':
    if not args.level:
        if not args.simple_rep or not args.ref:
            sys.stderr.write('Please specify -sr and -R when region !="ALU".\n'); sys.exit()
        simpleRep_dict = {}
        START_2 = time()
        for line in open(args.simple_rep, 'r'):
            if line.split('\t')[5].startswith('chr'): simpleRep_dict.setdefault(line.split('\t')[5], []).append([int(line.split('\t')[6]), int(line.split('\t')[7])])
            else: simpleRep_dict.setdefault('chr'+line.split('\t')[5], []).append([int(line.split('\t')[6]), int(line.split('\t')[7])])
        END_2 = time()
        sys.stderr.write('Information of simple repeats has been loaded. Running time: %s s.\n' % (END_2-START_2))
        START_3 = time()
        sa_dict = {} # dictionary for splicing artefacts and transcript region
        for _t in gtf_info.keys():
            exon_s = [x[0]+1 for x in gtf_info[_t][2][1:-1]]; exon_e = [x[1] for x in gtf_info[_t][2][:-2]]; chrom = 'chr'+gtf_info[_t][5]
            if chrom not in sa_dict: sa_dict[chrom] = []
            sa_region = list(zip(exon_e, exon_s))
            for _sr in range(len(sa_region)):
                if [sa_region[_sr][0]-args.splice_nt, sa_region[_sr][0]+args.splice_nt] in sa_dict[chrom]: continue
                else: sa_dict[chrom].append([sa_region[_sr][0]-args.splice_nt, sa_region[_sr][0]+args.splice_nt])
        target_site = {}
        END_3 = time()
        sys.stderr.write('GTF information has been loaded. Running time: %s s.\n' % (END_3-START_3))
    # else: 
    #     simpleRep_dict = None; sa_dict = None
    else:
        target_site = {}
        for x in open(args.level, 'r'):
            target_site.setdefault(x.split('\t')[0], []).append((int(x.split('\t')[1]), x.strip().split('\t')[2]))
        simpleRep_dict = None; sa_dict = None
        # snpdb_dict = {}
        # for snpdb in args.snpdb.split(','):
        #     for snp in open(snpdb, 'r'):
        #         snpdb_dict.setdefault(snp.split('\t')[0], []).append(int(snp.strip().split('\t')[1]))
        # for _c in snpdb_dict:
        #     snpdb_dict[_c] = sorted(snpdb_dict[_c])
    if args.INDEL: indel_bcf = VariantFile(args.INDEL)
    else: indel_bcf = None
    END = time()
    sys.stderr.write('Basic information has been loaded. Running time: %s s.\n' % (END-START))

    if args.p > 1:
        sys.stderr.write('\n################### Step 2: Multiprocessing #################\n')
        sys.stderr.write('Start to separate input File.\n')
        START_SEP = time()
        total_sites = int(list(os.popen('wc -l %s' % (args.input)))[0].split(' ')[0])
        sys.stderr.write('ALL SITES: '+str(total_sites)+'\n')
        separate = int(total_sites/args.p)
        # prepare the input file for each subprocess
        num = 0; file_idx = 0; lineStore = []
        for line in open(args.input, 'r'):
            num += 1; lineStore.append(line)
            if num == separate:
                out_idx = open(args.input+'.'+str(file_idx), 'w')
                for _line in lineStore:
                    out_idx.write(_line)
                out_idx.close()
                file_idx += 1; num = 0; lineStore = []
        out_idx_final = open(args.input+'.'+str(file_idx), 'w')
        for _line in lineStore:
            out_idx_final.write(_line)
        out_idx_final.close()
        END_SEP = time()
        sys.stderr.write('Input file has been separated. Running time: %s s.\n' % (END_SEP-START_SEP))

        from multiprocessing import Pool
        pool = Pool(processes=args.p)
        for i in range(args.p):
            pool.apply_async(main, args=(args.input+'.'+str(i), args.out+'.'+str(i), ), kwds={'simpleRep':simpleRep_dict, 'saDict':sa_dict, 'gtf_info':gtf_info, 'ref_File':args.ref, 'depth_t':args.depth, 'skip_t':args.skip, 'mismatch_t':args.mismatch, 'puzzle_t':args.puzzle, 'ahead':args.ahead, 'deletion_ratio':args.DR, 'BP':BP, 'BP_ratio':args.BP_ratio, 'indel_bcf':indel_bcf, 'homo_buffer':args.buffer, 'region':args.region, 'seq_error_p':10**((-1)*(round(args.quality/10, 1))), 'qual_t':args.quality, 'only_level':target_site}, error_callback=error_callback)
            # results.append(result)
        pool.apply_async(main, args=(args.input+'.'+str(args.p), args.out+'.'+str(args.p), ), kwds={'simpleRep':simpleRep_dict, 'saDict':sa_dict, 'gtf_info':gtf_info, 'ref_File':args.ref, 'depth_t':args.depth, 'skip_t':args.skip, 'mismatch_t':args.mismatch, 'puzzle_t':args.puzzle, 'ahead':args.ahead, 'deletion_ratio':args.DR, 'BP':BP, 'BP_ratio':args.BP_ratio, 'indel_bcf':indel_bcf, 'homo_buffer':args.buffer, 'region':args.region, 'seq_error_p':10**((-1)*(round(args.quality/10, 1))), 'qual_t':args.quality, 'only_level':target_site}, error_callback=error_callback)
        # results.append(result_f)
        # pool.start()
        pool.close()
        pool.join()
        sys.stderr.write('\n################### Multiple processing finished successfully #################\n')
        #if not args.level:
        #    os.system("cat %s.* | grep -v 'CHROM' | sed '1i CHROM\tPOS\tREADS_NUM\tREF\tALT\tEDITLEVEL\tCOVERAGE\tCOVERAGE_noSKIP\tSTRAND\tTRANSCRIPT\tGENE' > %s" % (args.out, args.out))
        #else:
        if args.level:
            os.system("cat %s.* | grep -v 'CHROM' | sed '1i #chrom\tposition\tstrand\tcoverage\teditedreads\teditlevel' > %s" % (args.out, args.out))
            os.system('rm %s.*' % args.out)
            os.system('rm %s.*' % args.input)
    else:
        sys.stderr.write('\n################### Step 2: Single process #################\n')
        main(args.input, args.out, simpleRep=simpleRep_dict, saDict=sa_dict, gtf_info=gtf_info, ref_File=args.ref, depth_t=args.depth, skip_t=args.skip, mismatch_t=args.mismatch, puzzle_t=args.puzzle, ahead=args.ahead, deletion_ratio=args.DR, BP=BP, BP_ratio=args.BP_ratio, indel_bcf=indel_bcf, homo_buffer=args.buffer, region=args.region, seq_error_p=10**((-1)*(round(args.quality/10, 1))), qual_t=args.quality, only_level=target_site)
        sys.stderr.write('\n################### Finished successfully #################\n')

def error_callback(error):
    sys.stderr.write('Error info: %s\n' % (error))

def gtf_table_trans(gtf_file):

    anno_dict = {}
    for line in open(gtf_file, 'r'):
        line_info = line.strip().split('\t')
        trans_strand = line_info[3]
        gene_region = [int(line_info[4]), int(line_info[5])]
        CDS_region = [int(line_info[6]), int(line_info[7])]
        exon_region = list(zip([int(x) for x in line_info[9].split(',')[:-1]], [int(x) for x in line_info[10].split(',')[:-1]]))
        gene_id = [line_info[-3], line_info[-2]] # (gene_id, gene_symbol)
        trans_type = line_info[-1]
        chrom = line_info[2]
        length = sum(starmap(lambda x,y:y-x, exon_region))
        anno_dict[line_info[1]] = [gene_region, CDS_region, exon_region, trans_strand, gene_id, chrom, trans_type, length]

    return(anno_dict)

def main(inputFile, outFile, simpleRep=None, saDict=None, gtf_info=None, ref_File=None, depth_t=2, skip_t=0.4, mismatch_t=2, puzzle_t=0.2, ahead=5, deletion_ratio=0.1, BP=50, BP_ratio=0.05, indel_bcf=None, homo_buffer=None, region='ALU', seq_error_p=0.05, qual_t=7, only_level=None):

    a = 0
    sys.stderr.write('Start analyzing......\n')
    out_file = open(outFile, 'w')
    ref_genome = FastaFile(ref_File)
    title_list = ['CHROM', 'POS', 'READS_NUM', 'REF', 'ALT', 'EDIT_LEVEL']
    title_prefix = ['QUAL_MEAN', 'QUAL_STD', 'DEL_NUM', 'DEL_RATE', 'DEL_LEN', 'INS_NUM', 'INS_RATE', 'INS_LEN', 'DEL_SITE_NUM', 'DEL_SITE_RATIO', 'DEPTH', 'BP', 'SKIP', 'PUZZLE_NUM', 'SR', 'HOMO', 'SPLICE', 'SEQ_ERROR_P']
    title_list.extend([x+'_1' for x in title_prefix])
    title_list.extend([x+'_mid' for x in title_prefix])
    title_list.extend([x+'_3' for x in title_prefix])
    title_list.append('MOTIF')
    out_file.write('\t'.join(title_list)+'\n')
    inputFile_file = open(inputFile, 'r')
    total_sites = int(list(os.popen('wc -l %s' % (inputFile)))[0].split(' ')[0])
    sys.stderr.write('Total sites for inputFile: %s.\n' % (total_sites))

    ALL_START_TIME = time(); START_TIME = time()
    output_num = 0
    indel_info_dict = {}; match_info_dict = {}
    for line in inputFile_file:
        line_info = line.strip().split('\t')
        a += 1
        if a > 0:
            END_TIME = time()
            sys.stderr.write('SINGLE_SITE_TIME: %s s.\n' % (END_TIME-START_TIME)); START_TIME = time()
        if a % 1000000 == 0:
            ALL_END_TIME = time()
            sys.stderr.write('%s sites have been analysed. (%s) Time: %s s.\n' % (a, str(round(a/total_sites, 4)*100)+'%', ALL_END_TIME-ALL_START_TIME))
            ALL_START_TIME = time()
        chrom = line_info[0]; pos = int(line_info[1]); ref = line_info[2]; depth = int(line_info[3]); match = line_info[4]; qual = line_info[5]; base_position = [int(x) for x in line_info[6].split(',') if x != '*']
        # if pos == ref_genome.get_reference_length(chrom): continue # last site of the chromosome
        # if chrom+'_'+str(pos) != 'chr22_22674621': continue
        sys.stderr.write('Current site: '+chrom+'_'+str(pos)+'\n')
        if depth == 0: sys.stderr.write('Depth of %s is 0.\n' % (chrom+'_'+str(pos))); continue
        if base_position: bp_ratio = min(base_position)
        else: bp_ratio = 'NA'
        # if BP != 'NA':
        #     if len([x for x in base_position if x < BP]) / len(base_position) > BP_ratio: sys.stderr.write('Site %s was covered in the head of too many reads.\n' % (chrom+'_'+str(pos))); continue
        if indel_bcf: # if there is at least one INDEL in the ahead region
            if [indel_info.pos for indel_info in indel_bcf.fetch(chrom, pos-ahead, pos) if indel_info.info['INDEL']]: 
                sys.stderr.write('Site %s has INDEL nearby\n' % (chrom+'_'+str(pos))); continue
        #if chrom+'_'+str(pos) not in ['chr2_'+str(x) for x in range(24223390, 24223400)]: continue
        #if chrom+'_'+str(pos) != 'chr2_99812857': continue
        # sys.stderr.write('\nSITE: %s:%s\n' % (chrom, pos))
        if only_level:
            for x in only_level[chrom]:
                if x[0] > pos: break
                elif x[0] < pos: continue
                else: pos_site = [x for x in only_level[chrom] if x[0] == pos]; break
            if not pos_site: continue
            else: pos_site = pos_site[0]
        #! if depth < depth_t: continue # depth checking
        match = match.upper()
        S_MATCH_INFO = time()
        match_count, confused_num, mismatch_num, indel_info, qual_anno, del_sum, ins_sum = match_info_parse(match, ref, chrom+'_'+str(pos), qual)
        E_MATCH_INFO = time()
        # sys.stderr.write('MATCH_INFO_TIME: '+str(E_MATCH_INFO-S_MATCH_INFO)+' s.\n')
        # match_count: number of '.,ATCGatcgNn*><'; confused_num: indel+head+end+"*"; mismatch_num: ATCGatcg; indel_info: indel
        # sys.exit()
        if chrom not in indel_info_dict.keys() and len(indel_info_dict) > 0:
            indel_info_dict.clear()
        indel_info_dict.setdefault(chrom, []).append((pos, indel_info))
        if abs(indel_info_dict[chrom][0][0] - pos) > ahead: del indel_info_dict[chrom][0]
        deletion_num_ahead = deletion_ahead_check(indel_info_dict[chrom], ahead, pos) # to check whether there is some deletion(s) covering target site
        # sys.stderr.write('match_count: %s; confused_num: %s; mismatch_num: %s; deletion_num_ahead: %s\n' % (match_count, confused_num, mismatch_num, deletion_num_ahead))
        snv_count = sum(match_count.values())-(match_count['>']+match_count['<']+match_count['*']) # '*.,acgtACGT'
        if 'N' in match_count: snv_count -= match_count['N']
        if 'n' in match_count: snv_count -= match_count['n']
        skip_value = round((match_count['>']+match_count['<'])/depth, 4)
        #! if (match_count['>']+match_count['<'])/depth > skip_t: sys.stderr.write('Site %s has many skipped reads.\n' % (chrom+'_'+str(pos))); continue
        # sys.stderr.write('%s_%s: %s\n' % (chrom, pos, snv_count+confused_num+deletion_num_ahead))

        # filter sites
        if not only_level: # currently need not to take the mismatch number in consideration
            # depth for mismatch base
            S_MATCH_INFO = time()
            puzzle_value = confused_num+deletion_num_ahead
            #! if (confused_num+deletion_num_ahead)/depth > puzzle_t: sys.stderr.write('Site %s has many puzzled mapped reads.\n' % (chrom+'_'+str(pos))); continue # too many reads annotated as confused (INDEL +/-, puzzled *)
            if len(mismatch_num) == 1 and mismatch_num[0][1] < mismatch_t: mismatch_num = [] # sys.stderr.write('Site %s does not have enough mismatch.\n' % (chrom+'_'+str(pos))); continue # not enough support for the major mismatch
            mismatch_num = sorted([_m for _m in mismatch_num if _m[1] >= mismatch_t], key=lambda x:x[1], reverse=True) # ranked according to reads number
            E_MATCH_INFO = time()
            #! if deletion_num_ahead/(snv_count+confused_num+deletion_num_ahead) > deletion_ratio: sys.stderr.write('Site %s is covered with many DELTIONs.\n' % (chrom+'_'+str(pos))); continue

        # homologous and simple repeat filtration
        # if region != 'ALU' and not only_level:
        if not only_level:
            S_MATCH_INFO = time()
            if chrom not in simpleRep: continue
            # not in simple repeat regions
            if [x for x in simpleRep[chrom] if x[0] <= pos <= x[1]]: simple_repeat = 1
            else: simple_repeat = 0
            # not containing homopolymer
            if homo_detect(ref_genome, homo_buffer, chrom, pos): homopolymer = 1
            else: homopolymer = 0
            E_MATCH_INFO = time()
            # sys.stderr.write('SIMPLE_HOMO_TIME: '+str(E_MATCH_INFO-S_MATCH_INFO)+' s.\n')
            # BLAT analysis

        # transcript annotation
        # if not only_level:
            # tmp_trans_id = []; tmp_gene_id = []
            # for x in gtf_info.keys(): # for each transcript
            #     if 'chr'+gtf_info[x][5] != chrom and gtf_info[x][5] != chrom: continue
            #     if gtf_info[x][0][0] < pos <= gtf_info[x][0][1]: tmp_trans_id.append(x); tmp_gene_id.append(gtf_info[x][4][0])
            # if len(set(tmp_gene_id)) > 1: sys.stderr.write('Site %s was mapped to multiple genes.\n' % (chrom+'_'+str(pos))); #! continue # annotated with multiple genes
            # # tmp_trans_id = [x for x in gtf_info.keys() if gtf_info[x][0][0] < rec.pos <= gtf_info[x][0][1]]
            # if not tmp_trans_id: trans_id = 'NA'; sys.stderr.write('Site %s is in INTERGENIC region.\n' % (chrom+'_'+str(pos))); continue # in intergenic region
            # else: trans_id = sorted(tmp_trans_id, key=lambda x:gtf_info[x][-1], reverse=True)[0] # the longest transcript covering target site
            # if trans_id == 'NA': strand_trans = "+" #! continue #strand_trans = '+', to modify later
            # else: strand_trans = gtf_info[trans_id][3]
        else:
            strand_trans = pos_site[1]
        # if region != 'ALU' and not only_level:
        if not only_level: 
            # not in splicing artefects
            S_MATCH_INFO = time()
            if chrom in saDict: # otherwise maybe only one exon for target transcript or target site is in intergenic region
                splice_nt_ori = nearest_SJ(saDict[chrom], pos)
                if splice_nt_ori >= 20: splice_nt = 20
                elif splice_nt_ori <= -20: splice_nt = -20
                else: splice_nt = splice_nt_ori
                #! if [x for x in saDict[chrom] if x[0] <= pos <= x[1]]: sys.stderr.write('Site %s is in splice sites.\n' % (chrom+'_'+str(pos))); continue
            else:
                splice_nt = 'NA'
            E_MATCH_INFO = time()
            # sys.stderr.write('TRANS_ANNO_TIME: '+str(E_MATCH_INFO-S_MATCH_INFO)+' s.\n')

        # output
        if only_level:
            if strand_trans == '+':
                y = sum([x[1] for x in mismatch_num if x[0].upper() == 'G'])
            else:
                y = sum([x[1] for x in mismatch_num if x[0].lower() == 'c'])
            if snv_count+confused_num+deletion_num_ahead == 0:
                out_file.write('%s\t%s\t%s\t%s\t%s\t%s\n' % (chrom, pos, strand_trans, (snv_count+confused_num+deletion_num_ahead), y, 'N/A')); continue
            else:
                out_file.write('%s\t%s\t%s\t%s\t%s\t%s\n' % (chrom, pos, strand_trans, (snv_count+confused_num+deletion_num_ahead), y, round(y/(snv_count+confused_num+deletion_num_ahead),4))); continue
        else: # without filtration according to strand
            to_output = False
            S_MATCH_INFO = time()
            if not mismatch_num: mismatch_best = ['NA', 0] # no mismatch
            else: mismatch_best = mismatch_num_correction(mismatch_num)
            #alt_qual = ','.join([str(10**((-1)*x[1]/10)) for x in qual_anno if x[0].upper() == mismatch_best[0]])
            #alt_qual_mean
            qual_level = [10**((-1)*x[1]/10) for x in qual_anno]
            qual_mean = np.mean(qual_level); qual_std = np.std(qual_level)
            if not mismatch_num: seq_error_pvalue = 'na' # no mismatch
            else: seq_error_pvalue = round(binom.cdf(k=mismatch_best[1], n=snv_count+confused_num+deletion_num_ahead, p=seq_error_p), 4)
            if depth-(match_count['>']+match_count['<']) == 0:
                match_info_dict.setdefault(chrom, []).append((pos, [ref, mismatch_best, snv_count, confused_num, deletion_num_ahead, match_count, qual_mean, qual_std, '\t'.join([str(x) for x in del_sum]), '\t'.join([str(x) for x in ins_sum]), deletion_num_ahead, 'na', depth-(match_count['>']+match_count['<']), bp_ratio, skip_value, puzzle_value, simple_repeat, homopolymer, splice_nt, seq_error_pvalue])) # depth=depth-(match_count['>']+match_count['<'])
            else:
                match_info_dict.setdefault(chrom, []).append((pos, [ref, mismatch_best, snv_count, confused_num, deletion_num_ahead, match_count, qual_mean, qual_std, '\t'.join([str(x) for x in del_sum]), '\t'.join([str(x) for x in ins_sum]), deletion_num_ahead, round(deletion_num_ahead/(depth-(match_count['>']+match_count['<'])), 4), depth-(match_count['>']+match_count['<']), bp_ratio, skip_value, puzzle_value, simple_repeat, homopolymer, splice_nt, seq_error_pvalue]))
            #sys.stderr.write('HAS information output: %s\n' % match_info_dict[chrom])
            #! if binom.cdf(k=mismatch_best[1], n=snv_count+confused_num+deletion_num_ahead, p=seq_error_p) < 0.05: sys.stderr.write('Site %s with mismatch %s maybe generated by sequencing error.\n' % (chrom+'_'+str(pos), x)); continue # sequence error
            if all(map(lambda x:x in [x[0] for x in match_info_dict[chrom]], [pos-2, pos-1, pos])):
                sys.stderr.write('mid_site_info: '+str([x for x in match_info_dict[chrom] if x[0] == pos-1][0])+'\n')
                sys.stderr.write('left_info_out: '+str([x for x in match_info_dict[chrom] if x[0] == pos-2][0])+'\n')
                sys.stderr.write('right_info_out: '+str([x for x in match_info_dict[chrom] if x[0] == pos][0])+'\n')
                mid_site_info = [x for x in match_info_dict[chrom] if x[0] == pos-1][0][1]
                if mid_site_info[1][0] == 'NA': continue # mid site must have mismatch
                sys.stderr.write('xxx_mid_site_info: '+str([x for x in match_info_dict[chrom] if x[0] == pos-1][0])+'\n')
                sys.stderr.write('xxx_left_info_out: '+str([x for x in match_info_dict[chrom] if x[0] == pos-2][0])+'\n')
                sys.stderr.write('xxx_right_info_out: '+str([x for x in match_info_dict[chrom] if x[0] == pos][0])+'\n\n')
                basic_info = [chrom, pos-1, str(mid_site_info[5]['.']+mid_site_info[5][','])+','+str(mid_site_info[1][1]), mid_site_info[0], mid_site_info[1][0], round(mid_site_info[1][1]/(mid_site_info[2]+mid_site_info[3]+mid_site_info[4]),4)] # CHROM, POS, READS_NUM, REF, ALT, EDIT_LEVEL
                mid_info_out = mid_site_info[6:] # QUAL_MEAN, QUAL_STD, DEL_NUM, DEL_RATE, DEL_LEN, INS_NUM, INS_RATE, INS_LEN, DEL_SITE_NUM, DEL_SITE_RATIO, DEPTH, BP, SKIP, PUZZLE_NUM, SR, HOMO, SPLICE, SEQ_ERROR_P
                left_info_out = [x for x in match_info_dict[chrom] if x[0] == pos-2][0][1][6:] # QUAL_MEAN, QUAL_STD, DEL_NUM, DEL_RATE, DEL_LEN, INS_NUM, INS_RATE, INS_LEN, DEL_SITE_NUM, DEL_SITE_RATIO, DEPTH, BP, SKIP, PUZZLE_NUM, SR, HOMO, SPLICE, SEQ_ERROR_P
                right_info_out = [x for x in match_info_dict[chrom] if x[0] == pos][0][1][6:] # QUAL_MEAN, QUAL_STD, DEL_NUM, DEL_RATE, DEL_LEN, INS_NUM, INS_RATE, INS_LEN, DEL_SITE_NUM, DEL_SITE_RATIO, DEPTH, BP, SKIP, PUZZLE_NUM, SR, HOMO, SPLICE, SEQ_ERROR_P
                basic_info.extend(left_info_out)
                basic_info.extend(mid_info_out)
                basic_info.extend(right_info_out)
                basic_info.append(ref_genome.fetch(chrom, pos-3, pos).upper())
                match_info_dict[chrom][:] = [x for x in match_info_dict[chrom] if x[0] >= pos-2]
                sys.stderr.write('match_info_dict: '+str(match_info_dict[chrom])+'\n')
                out_file.write('\t'.join([str(x) for x in basic_info])+'\n')
                to_output = True
                # sys.exit()
            # if not snv_out: continue
            if to_output:
                E_MATCH_INFO = time()
                sys.stderr.write('OUTPUT_TIME: '+str(E_MATCH_INFO-S_MATCH_INFO)+' s.\n')
                # output_num += 1
                # if output_num > 5: sys.exit()
        
    out_file.close()

def match_info_parse(match_info, ref, Pos, qual):

    ### for parsing indel information, e.g. ',+6CTGCAG-3CTGc', ',+1Aa'
    #! for test: 
    #! head: ['^\.', '^],', '^\A', '^\c+3ATT']; end: ['.$', '.$', 'A$']; indel: ['c-1c', '.+3acc', 'g-2ac', '.+3acc-2ac']; snv_m: ['.', '.', ',', ',', 'c', 'c', '.']
    # match_info = '>><<<><<<<<<<><<<<<<<<<<><<<<<<<<<><><>><<<<<>>>><><<>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<><<<<<<<<<<<<<<<<<<<<<><<<<<<<<<<<>>>><>><>>>>><>>>,+2CCA+328TGCCTGGCCACTCCGGCAATTTCTTTATATAGGTATTCCTATCTTTTTTTTGAAAAACTAATATATCGGTATGGTCTTATATTAATAAGTGAATTAGTTGATTATGGACTTGAATCTGTTATAATATGTAGTAAATTCAGTTCTTCCTCTTTTTTCTCCGTTAATTTCCTTAGCTTATTTTCATTTATTTGTTTCTCTAAATGAACTTTTGACTAGCGTTCTGTCATGTTCCAAAAATAATATCTTACTGTGATTGAATCTATAAACGTATATGTTTTCTGACATTTTTGTAACTTCTACACAGAATTAATCTACTTGAAGAACGCTG.A+343TGCCTGGCCACTCCCAGCAATTTCTTTGTATGGTCTATCCTATCTTTTTTTGAAAAACTAATATATAGGTATGGTCTTATATTAATAAGTCAATTAGTTGATTATGGACTTAAATATGTTATAATATGTAGTAAATTCAGTTCTTCCTACTTTTTATTTCATAATTTCCTTAGCTTATTTTCATTTATTTGTTTATTCTAAATAAACTTTAGACACATTCTGTGTCATGTTCCAAAAATAATATCTTACTTTTGATTGGAATTATATAAACGTATATGTTTTCTGACATTTTTTGTAATTTCTACACAGAATTAATATTACTTGAAGAAACACTGAGCCACTG'
    # match_info = '>><<..,,^\.^],^\A^\c+3ATTc-1c.$.$A$G+3accg-2acc..+3acc-2ac'
    # match_info = '>.-12TTTTCTTTTCTTT'
    # match_info = '<<<>>>><<>.*.+230AACTTGGGAGGCTGAGGCAGGAGAATCTCTTGAACCTGGGAGGTGGAGGTTGCAGCGAGCTGAGATCACACCATTGCACTCCAGCCTGGGCAACAGAATTGAAACTCCATCTCAAAAAAAAAAAAAAAAAAAAAATTTGCTGGGTGTAGTGGTCTCATCTGTAATCCCAGCTACTGCCTGAGCAGCTGGGACTACAGTGCACACGCCCGGCAATTTTTTTTTTTTTTTTT-3AACT'.upper()
    # match_info = '<<><,-1t><.-3TTT.-4TTTT.-1T<>.-2TT,-5ttttt,,,-5ttttt,+1t,-2tt<.+4TTTT,-1t..+3TTT.+5TTTTT,-2tt,+7ttttttt,-2tt.-2TT.-4TTTT.-2TT.+2TT.+2TT,-1t.-3TTT,+9ttttttttt,+1t,-2tt,,,-2tt.-2TT,-1t,+6tttttt>.-12TTTTTTTTTTTT,+2at,-3ttt..-1T,-1t.+3TTT.+2TT..+4TTTT.+1T.-2TT.+5TTTTT.-3TTT..+204TTTTTTTTTTTTTTTTTTTTTGAGGCTGAGTCTCACTCTGTCGCCCAGGCTGGAGTGCAATGGAGCGATCTCGGCTCACTGCAACCCTCCGCCTGGGCGATGAGTGAGACTCAGCCTGCAAAAAAAAAAAAAAAAAAAAATTAGCCAGGTGTGGTGGTGCACCTGTAATCCCAGCTAGGAGGCAGGCCAGGCGCAACTGGCGAA-5TTTTT.+2TT.+10TTTTTTTTTT.+2TT.+3CTT.-1T,+3ttt,+9ttttttttt.-3TTT.-1T,-1t,-2tt,-1t,-1t,+1t,+5ttttt,-6tttttt,,-4tttt,-3ttt,,+1t,+5ttttt,+1t,-4tttt,,,,+1t,-1t,-1t,+1t,+4tttt,-1t,+4tttt,,,-7ttttttt,+4tttt,-2tt,+1t,,-3ttt,-2tt,+4tttt.,-3ttt,-2tt,-4tttt,-2tt,-1t,-1t,-2tt,+7ttttttt,-2tt,-3ttt,-4tttt,-4tttt,-12tttttttttttt,.+5TTTTT,...+1T.+6TTTTTT..-1T.+3TTT.-1T.-4TTTT.+1T.-2TT.-4TTTT.-3TTT.-1T.+2TT.+4TTTT.+4TTTT.-2TT..-5TTTTT.-3TTT..-6TTTTTT.-2TT..+7TTTTTTT.-1T.+4TTTT...+4TTTT..-2TT.-6TTTTTT.-2TT.-3TTT.-2TT.-3TTT.+1T.+8TTTTTTTT..-3TTT.-3TTT.+8TTTTTTTT.+7TTTTTTT'.upper()
    # match_info = '>><<<<<><<<<<<<<<<<<<<<<<<<<><<<<<<<<<<<<<<<<<<<<<<<<<<<<<><<<<<<<><><><<<<<<<<<><<<><>>><<>><<<><><<<<<>>><>><<<<>><<<<<<<<<<>><><<<<<<<<<>>.<>>>><<<>>,>>>>>>>>>>>>>>><>><<><>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<>>>><>>>>>>><<<<><><<<<>><<<>>>>><<><>.+1AA'.upper()
    # qual = '1(,+5/+507)*3;0):3+5.3*,--2;683-:A:/..:=*1/081*(0)*(..6,006/84)8/);<203.:9+37692)+/4/.52*674-58614>21.8,6*,(366+254550+845(2*1).+4..0)6=,**0)6311/36,(/8*81+4.?97-:3605(*@4)0,38121001/.67)(2.1*;1/03424(/.408-3,*,=9:856*3./46,2.614.;6.634:.*+.>1*7;(9600(78;+(9))?8(43592.=)4>/-)8-28,0/0:/8/)-,;/.90/.47-,63<--/+6:2((:--55;547)1,5()718?{{JAD/>85A3<+2?B/<){A7B?:@5;FC1A>946@@8?3>>E<>313E;B>@=)@=6<D?C:1>.;77{;?@=<9>B6+>(*3C5='
    # qual = '(>{)9*026*<)-3.);54/,31705/+2(33,6,44257<0997,2814623+3/105/53-3//6+745076<98;?4;5:@2:6678750923-92641585/5787)77?0518)=3/(-30304331-1./0.4366(261*30-504++.6331601(2/('
    # qual = '68/9*46917.8+.92)(:;941:=6<<-.:(7;68*;4)18/28;4:<:?6:8*7;9:.8-:6(;.8547277899;:01*7<4;<1.767=697,9/:945<143497+*<4/8::785+287<666+<859.875?28:485/74;9,7*36634,1(707,5,;8=15(=>9;1*<5,7;2672/:343(>:3;25858-);2560;53058.43,-*:5/0-623296:*83/62,36;.7;4:76221<44313=86*:-*96834)5'
    # qual = '5<@)782?017</.'
    # qual = ':5>=3>33171;5B59/,91<653307527:{B?F?=I@C<6ACAAB;=B?=?;<=DA;=I8;'
    # qual = '1(,+5/+507)*3;0):3+5.3*,--2;683-:A:/..:=*1/081*(0)*(..6,006/84)8/);<203.:9+37692)+/4/.52*674-58614>21.8,6*,(366+254550+845(2*1).+4..0)6=,**0)6311/36,(/8*81+4.?97-:3605(*@4)0,38121001/.67)(2.1*;1/03424(/.408-3,*,=9:856*3./46,2.614.;6.634:.*+.>1*7;(9600(78;+(9))?8(43592.=)4>/-)8-28,0/0:/8/)-,;/.90/.47-,63<--/+6:2((:--55;547)1,5()718?{{JAD/>85A3<+2?B/<){A7B?:@5;FC1A>946@@8?3>>E<>313E;B>@=)@=6<D?C:1>.;77{;?@=<9>B6+>(*3C5='
    # Pos = 'x'

    snv_m = []; tmp_idx = []; order_parse = []
    head = re.findall(r'\^.[.,ATCGatcgNn][+-]\w+|\^.[.,ATCGatcgNn]', match_info); head_final = []
    head_insert = [x for x in head if '+' in x] # head_insert, '^]c+3ATTc'
    if head_insert:
        for _hi in frozenset(head_insert): # _hi: ^\\c+3ATTc
            modi_hi = _hi.replace('\\', '\\\\').replace('^', '\^').replace('?','\?').replace('[', '\[').replace('*', '\*').replace('+', '\+').replace('.', '\\.') # '\\^\\\\c\\+3ATTc'
            _hi_start = [x.start() for x in re.finditer(modi_hi, match_info)][0] # 17
            _hi_ins_start = [x.start() for x in re.finditer(r'.[+-]\w+', _hi)][0] # 2
            _hi_ins_len = int(re.findall(r'[0-9]+', _hi)[0]) # 3
            tmp_idx.extend(list(range(_hi_start, _hi_start+4+len(str(_hi_ins_len))+_hi_ins_len)))
            order_parse.append([_hi_start, match_info[_hi_start:_hi_start+4+len(str(_hi_ins_len))+_hi_ins_len]]) # store results, 4:'^..[+-]', len(str(_hi_ins_len)): len('119')
            head_final.append(match_info[_hi_start:_hi_start+4+len(str(_hi_ins_len))+_hi_ins_len])
    for _h in frozenset(head):
        if _h in head_insert: continue
        modi_h = _h.replace('\\', '\\\\').replace('^', '\^').replace('?','\?').replace('[', '\[').replace('*', '\*').replace('+', '\+').replace('.', '\\.') 
        _h_start = [x.start() for x in re.finditer(modi_h, match_info) if x.start() not in tmp_idx]
        for _ in _h_start:
            tmp_idx.extend([_+_x for _x in range(len(_h))])
            order_parse.append([_, _h])
            head_final.append(_h)
    end = re.findall(r'[.,ATCGatcgNn]\$', match_info) 
    for _end in frozenset(end):
        modi_end = _end.replace('\\', '\\\\').replace('$', '\$').replace('?','\?').replace('[', '\[').replace('*', '\*').replace('.', '\\.')
        _e_start = [x.start() for x in re.finditer(modi_end, match_info) if x.start() not in tmp_idx]
        for _ in _e_start:
            tmp_idx.extend([_+_x for _x in range(len(_end))])
            order_parse.append([_, _end])

    indel_ori = re.findall(r'[+-]\w+', match_info); indel = []
    # sys.stderr.write('indel_ori: %s\n' % (indel_ori))
    indel_order = []
    for _ind in dedup_list(indel_ori):
        modi_indel = _ind.replace('+', '\+').replace('.', '\\.').replace('?','\?').replace('[', '\[').replace('*', '\*')
        # sys.stderr.write('modi_indel: %s\n' % (modi_indel))
        _ind_start = [x.start()-1 for x in re.finditer(modi_indel, match_info)]
        for _is in _ind_start:
            indel_order.append([_is, _ind])
    for _is, _ind in sorted(indel_order, key=lambda x:x[0]):
        combine_ahead = False
        if _is in tmp_idx: 
            if _is != tmp_idx[-1]: continue # the tested INDEL has been analysed
            else: combine_ahead = True
        _ind_x = match_info[_is]+_ind
        _ind_x_len = (int(re.findall(r'[0-9]+', _ind)[0]), re.findall(r'[a-zA-Z]+', _ind)[0])
        # sys.stderr.write('_ind_x: %s\n_ind_x_len: %s\n_ind_x_len[0] == len(_ind_x_len[1]): %s\n' % (_ind_x, _ind_x_len, _ind_x_len[0] == len(_ind_x_len[1])))
        if _ind_x_len[0] == len(_ind_x_len[1]): # INDEL 
            if not combine_ahead:
                order_parse.append([_is, _ind_x]); tmp_idx.extend([_is+_x for _x in range(len(_ind_x))]); indel.append(_ind_x)
            else:
                order_parse[-1][-1] += _ind_x[1:]; tmp_idx.extend([_is+_x for _x in range(len(_ind_x))]); indel[-1] += _ind_x[1:]
        else: # the last few bases are SNV
            if not combine_ahead:
                order_parse.append([_is, match_info[_is:_is+2+len(str(_ind_x_len[0]))+_ind_x_len[0]]]); tmp_idx.extend(list(range(_is,_is+2+len(str(_ind_x_len[0]))+_ind_x_len[0]))); indel.append(match_info[_is:_is+2+len(str(_ind_x_len[0]))+_ind_x_len[0]])
            else:
                order_parse[-1][-1] += match_info[_is+1:_is+2+len(str(_ind_x_len[0]))+_ind_x_len[0]]; tmp_idx.extend(list(range(_is+1,_is+2+len(str(_ind_x_len[0]))+_ind_x_len[0]))); indel[-1] += match_info[_is+1:_is+2+len(str(_ind_x_len[0]))+_ind_x_len[0]]
            # snv2add = _ind_x_len[1][_ind_x_len[0]:]
            # for _x in range(len(snv2add)):
            #     snv_m.append(snv2add[_x])
            #     order_parse.append([_is+2+len(str(_ind_x_len[0]))+_ind_x_len[0]+_x, snv2add[_x]])

    for _idx in range(len(match_info)):
        if _idx in tmp_idx: continue
        snv_m.append(match_info[_idx])
        order_parse.append([_idx, match_info[_idx]])

    # sys.stderr.write('match_info: %s; indel: %s; head: %s; end: %s; snv_m: %s\n' % (match_info, indel, head_final, end, snv_m))

    ### for quality calculation
    # order_parse = order_of_parse_information(indel, head, end, match_info)
    qual_order = sorted(order_parse, key=lambda x:x[0])
    qual_anno = list(zip([x[1] for x in qual_order], [ord(x)-33 for x in list(qual)])) # [(annotation, quality)]
    # sys.stderr.write('qual: %s\norder_parse: %s\nqual_anno: %s\n' % (qual, sorted(order_parse, key=lambda x:x[0]), qual_anno))
    if len(order_parse) != len(qual): sys.stderr.write('Error at Pos %s (quality)\n' % (Pos)); sys.exit(); return([('.', 0)])

    confused_num = len(indel)
    parse_rlt = []; parse_rlt.extend(indel)
    parse_rlt.extend(head_final); confused_num += len(head_final)
    parse_rlt.extend(end); confused_num += len(end)
    parse_rlt.extend(snv_m)
    #sys.stderr.write('parse_rlt: '+str(parse_rlt)+'\n')
    #sys.stderr.write('Pos: %s\nindel_length: %s\nindel: %s\nhead: %s\nend: %s\nsnv_m: %s\n' % (Pos, indel_length, indel, head, end, snv_m))
    # sys.stderr.write('len(indel): %s; depth: %s\n' % (len(indel), depth))
    if len(parse_rlt) != len(qual): sys.stderr.write('Error at Pos %s\n' % (Pos)); sys.exit(); return([('.', 0)])
    del_number = len([x for x in parse_rlt if '-' in x]); del_len = [int(re.findall(r'-[0-9]+', x)[0][1:]) for x in parse_rlt if '-' in x]
    ins_number = len([x for x in parse_rlt if '+' in x]); ins_len = [int(re.findall(r'\+[0-9]+', x)[0][1:]) for x in parse_rlt if '+' in x]
    del_sum = [del_number, round(del_number/len(qual), 4), np.mean(del_len)]
    ins_sum = [ins_number, round(ins_number/len(qual), 4), np.mean(ins_len)]

    ### Counting mismatch number
    snv_dict = Counter([x.upper() for x in snv_m])
    indel_mismatch = [x[0].upper() for x in indel if x[0] in list('acgtACGT')]
    for _i in indel_mismatch:
        if _i in snv_dict: snv_dict[_i] += 1
        else: snv_dict[_i] = 1
    if set(snv_dict.keys()) & set(['A', 'C', 'G', 'T', 'a', 'c', 'g', 't']):
        mismatch_num = [(_k, snv_dict[_k]) for _k in snv_dict.keys() if _k in ['A', 'C', 'G', 'T', 'a', 'c', 'g', 't'] and _k.upper() != ref.upper()]
    else:
        mismatch_num = [('.', 0)]
    confused_num += snv_dict['*']# + snv_dict['>'] + snv_dict['<']
    if 'N' in snv_dict: confused_num += snv_dict['N']
    if 'n' in snv_dict: confused_num += snv_dict['n'] 
    # sys.exit()

    return(snv_dict, confused_num, mismatch_num, indel, qual_anno, del_sum, ins_sum) 
    ### output of the function
    # snv_dict: '>' + '<' + '*' + '.' + ',' + 'acgtACGT'
    # confused_num = len(indel)+len(head)+len(end)+snv_dict['*']
    # mismatch_num = len(snv_dict['acgtACGT'])

def dedup_list(inputList):

    out_list = []
    for _l in inputList:
        if _l in out_list: continue
        out_list.append(_l)

    return(out_list)

def deletion_ahead_check(indel_info_dict, ahead, pos):
    
    deletion_num = 0
    if len(indel_info_dict) == 0: return(0)
    # sys.stderr.write('indel_info_dict: %s.\n' % (indel_info_dict))
    # sys.stderr.write('POS: %s; indel_info_dict: %s\n' % (pos, [x for x in indel_info_dict if 0 < pos-x[0] <= ahead]))
    for indel_info in indel_info_dict:
        dis = pos - indel_info[0]
        # sys.stderr.write('indel_info: '+str(indel_info)+'\n')
        if 0 < dis <= ahead: 
            # sys.stderr.write(indel_info)
            cover_pos_del = [x for x in indel_info[1] if int(re.findall(r'[0-9]+', x)[0]) >= dis and x[1] == '-'] # select those deletion (x[1][1] == '-') covering target site (>= dis)
            deletion_num += len(cover_pos_del)
            # sys.stderr.write('cover_pos_del: %s; deletion_num: %s\n' % (cover_pos_del, deletion_num))
        elif dis < 0: break
        elif dis > ahead or dis == 0: continue
    
    return(deletion_num)

def homo_detect(ref_genome, homo_buffer, chrom, pos):
    ### ATTENTION: for this part, further check is needed

    homo_len = homo_buffer
    seq = ref_genome.fetch(chrom, pos-homo_buffer, pos+homo_buffer+1)
    for x in range(len(seq)-homo_len+1):
        if len(set(list(seq[x:x+homo_len]))) == 1: return(True) # only one type of base, so in homopolymer
    return(False)

def mismatch_num_correction(mismatch_num):

    mismatch_out = []
    # without filtration according to strand information
    mismatch_out.append(('G', sum([x[1] for x in mismatch_num if x[0].upper() == 'G']))) 
    mismatch_out.append(('C', sum([x[1] for x in mismatch_num if x[0].upper() == 'C'])))
    mismatch_out.append(('A', sum([x[1] for x in mismatch_num if x[0].upper() == 'A'])))
    mismatch_out.append(('T', sum([x[1] for x in mismatch_num if x[0].upper() == 'T'])))
    if not mismatch_out: return([])
    return(sorted([x for x in mismatch_out if x[1] > 0], key=lambda x:x[1], reverse=True)[0]) # choose the mutation with the highest coverage. If multiple mutations have the same number, then choose the best one according to 'ACGT'

def nearest_SJ(saDict_chrom, pos):

    min_dis = 10000
    for x,y in saDict_chrom:
        if abs(x-pos) < abs(min_dis): min_dis = x-pos
        elif abs(y-pos) < abs(min_dis): min_dis = y-pos

    return(min_dis)

if __name__ == '__main__':
    arg(sys.argv)
